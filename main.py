# -*- coding: utf-8 -*-

import os
from google.cloud import documentai
from google.cloud import storage
import json
import time

from flask import Flask, request

app = Flask(__name__)

# Globaler Cache f√ºr verarbeitete Events (in Produktionsumgebung sollte Redis/Memcache verwendet werden)
processed_events = {}

@app.route("/", methods=["POST"])
def http_entrypoint():
    # Cloud Functions (2nd gen) und Cloud Run schicken das Event als JSON im Body
    event = request.get_json(force=True)
    
    # Event-ID f√ºr Deduplizierung extrahieren
    event_id = None
    if 'ce-eventid' in request.headers:
        event_id = request.headers['ce-eventid']
    elif 'eventId' in event:
        event_id = event['eventId']
    
    # Kontext ist optional, kann leer bleiben
    context = {}
    process_document_from_gcs(event, context, event_id)
    return ("", 204)

def process_document_from_gcs(event, context, event_id=None):
    """
    Diese Cloud Function wird durch einen Datei-Upload in einen Google Cloud Storage
    Bucket ausgel√∂st. Sie analysiert das Dokument mit Document AI und speichert
    den extrahierten Text in einem anderen Bucket.

    Args:
        event (dict): Das Event-Payload, das von Google Cloud bereitgestellt wird.
                      Es enth√§lt Details zur hochgeladenen Datei.
        context (google.cloud.functions.Context): Metadaten zum Event.
    """
    # --- 1. Konfiguration aus Umgebungsvariablen laden ---
    # Diese Variablen m√ºssen in der Google Cloud Function Konfiguration gesetzt werden.
    # Dies ist sicherer und flexibler als das Eintragen der Werte direkt in den Code.
    try:
        project_id = os.environ["GCP_PROJECT"]
        location = os.environ["PROCESSOR_LOCATION"]  # z.B. 'eu' oder 'us'
        processor_id = os.environ["PROCESSOR_ID"]
        output_bucket_name = os.environ["OUTPUT_BUCKET"]
    except KeyError as e:
        print(f"Fehler: Die Umgebungsvariable {e} ist nicht gesetzt!")
        # Beendet die Funktion, da die Konfiguration unvollst√§ndig ist.
        return

    # --- 2. Informationen zur Trigger-Datei aus dem Event auslesen ---
    input_bucket_name = event["bucket"]
    file_name = event["name"]
    content_type = event.get("contentType", "")

    print(f"‚úÖ Datei erkannt: gs://{input_bucket_name}/{file_name}")
    
    # Pr√ºfen, ob es sich um den Output-Bucket handelt - wenn ja, ignorieren
    if input_bucket_name == output_bucket_name:
        print(f"‚ö†Ô∏è Datei stammt aus dem Output-Bucket ({output_bucket_name}). Verarbeitung wird √ºbersprungen.")
        return

    # Pr√ºfen, ob die Datei bereits verarbeitet wurde (hat .txt Endung oder enth√§lt "_verarbeitet")
    if file_name.endswith('.txt') or '_verarbeitet' in file_name:
        print(f"‚ö†Ô∏è Datei '{file_name}' scheint bereits verarbeitet zu sein. Verarbeitung wird √ºbersprungen.")
        return

    # Sicherstellen, dass nur PDF-Dateien verarbeitet werden, um Fehler zu vermeiden.
    if not content_type == "application/pdf":
        print(f"‚ö†Ô∏è Datei '{file_name}' ist keine PDF-Datei ({content_type}). Verarbeitung wird √ºbersprungen.")
        return

    # --- 3. Document AI und Storage Clients initialisieren ---
    storage_client = storage.Client()
    
    # **WICHTIG: Zuerst pr√ºfen, ob die Ausgabedatei bereits existiert**
    # Das ist die dauerhafte Pr√ºfung, die unabh√§ngig vom Event-Cache funktioniert
    output_filename = f"{os.path.splitext(file_name)[0]}.txt"
    output_bucket = storage_client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_filename)
    
    if output_blob.exists():
        print(f"‚ö†Ô∏è Ausgabedatei '{output_filename}' existiert bereits. Verarbeitung wird √ºbersprungen.")
        return

    # Event-Deduplizierung - verhindert mehrfache Verarbeitung desselben Events (nur f√ºr kurze Zeit)
    if event_id:
        cache_key = f"{event_id}_{input_bucket_name}_{file_name}"
        current_time = time.time()
        
        # Pr√ºfen, ob dieses Event bereits verarbeitet wurde (Cache f√ºr 10 Minuten)
        if cache_key in processed_events:
            if current_time - processed_events[cache_key] < 600:  # 10 Minuten
                print(f"‚ö†Ô∏è Event {event_id} bereits verarbeitet. √úberspringe.")
                return
        
        # Event als verarbeitet markieren
        processed_events[cache_key] = current_time
        
        # Alte Eintr√§ge aus dem Cache entfernen (√§lter als 10 Minuten)
        to_remove = [k for k, v in processed_events.items() if current_time - v > 600]
        for k in to_remove:
            del processed_events[k]
        
        print(f"üîÑ Verarbeite Event {event_id}")

    # Pr√ºfen, ob es sich um den Output-Bucket handelt - wenn ja, ignorieren
    if input_bucket_name == output_bucket_name:
        print(f"‚ö†Ô∏è Datei stammt aus dem Output-Bucket ({output_bucket_name}). Verarbeitung wird √ºbersprungen.")
        return

    # Pr√ºfen, ob die Datei bereits verarbeitet wurde (hat .txt Endung oder enth√§lt "_verarbeitet")
    if file_name.endswith('.txt') or '_verarbeitet' in file_name:
        print(f"‚ö†Ô∏è Datei '{file_name}' scheint bereits verarbeitet zu sein. Verarbeitung wird √ºbersprungen.")
        return

    # Sicherstellen, dass nur PDF-Dateien verarbeitet werden, um Fehler zu vermeiden.
    if not content_type == "application/pdf":
        print(f"‚ö†Ô∏è Datei '{file_name}' ist keine PDF-Datei ({content_type}). Verarbeitung wird √ºbersprungen.")
        return

    # --- 3. Document AI und Storage Clients initialisieren ---
    # Es wird empfohlen, Clients innerhalb der Funktion zu initialisieren.
    storage_client = storage.Client()
    # WICHTIG: Den API-Endpunkt explizit basierend auf der Location setzen.
    # F√ºr EU-Region muss der Endpunkt korrekt sein
    if location == "eu":
        api_endpoint = "eu-documentai.googleapis.com"
    elif location == "us":
        api_endpoint = "us-documentai.googleapis.com"
    else:
        api_endpoint = f"{location}-documentai.googleapis.com"
    
    print(f"üîó Verwende API-Endpunkt: {api_endpoint}")
    opts = {"api_endpoint": api_endpoint}
    docai_client = documentai.DocumentProcessorServiceClient(client_options=opts)

    # Den vollst√§ndigen Pfad zum Document AI Prozessor zusammenbauen.
    processor_path = docai_client.processor_path(project_id, location, processor_id)
    print(f"üìç Prozessor-Pfad: {processor_path}")

    # --- 4. Dokument aus dem Storage laden und verarbeiten ---
    print(f"‚öôÔ∏è Verarbeite Dokument mit Prozessor: {processor_id}")
    try:
        # Die Datei aus dem Input-Bucket als Bytes herunterladen.
        input_bucket = storage_client.bucket(input_bucket_name)
        input_blob = input_bucket.blob(file_name)
        image_content = input_blob.download_as_bytes()

        # Das Dokument f√ºr die API-Anfrage vorbereiten.
        raw_document = documentai.RawDocument(
            content=image_content,
            mime_type=content_type,
        )

        # Die Anfrage an die Document AI API senden.
        request = documentai.ProcessRequest(
            name=processor_path,
            raw_document=raw_document
        )
        result = docai_client.process_document(request=request)
        document = result.document
        print("üìÑ Dokument erfolgreich verarbeitet.")

    except Exception as e:
        print(f"‚ùå Fehler bei der Document AI Verarbeitung: {e}")
        return

    # --- 5. Ergebnis in den Output-Bucket speichern ---
    # Strukturierte Daten aus dem trainierten Modell extrahieren
    extracted_data = {
        "document_info": {
            "filename": file_name,
            "processing_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "processor_id": processor_id,
            "total_pages": len(document.pages) if document.pages else 0
        },
        "entities": {},
        "raw_text": document.text
    }

    # Entit√§ten aus dem trainierten Modell extrahieren
    if document.entities:
        print(f"üîç {len(document.entities)} Entit√§ten gefunden:")
        for entity in document.entities:
            entity_type = entity.type_
            entity_value = entity.mention_text
            confidence = entity.confidence
            
            print(f"  - {entity_type}: {entity_value} (Konfidenz: {confidence:.2f})")
            
            # Entit√§t zu den extrahierten Daten hinzuf√ºgen
            if entity_type not in extracted_data["entities"]:
                extracted_data["entities"][entity_type] = []
            
            extracted_data["entities"][entity_type].append({
                "value": entity_value,
                "confidence": confidence,
                "normalized_value": entity.normalized_value.text if entity.normalized_value else None
            })
    else:
        print("‚ö†Ô∏è Keine Entit√§ten gefunden. M√∂glicherweise ist das Modell noch nicht trainiert.")

    # Zus√§tzliche Dokumenteigenschaften extrahieren
    if document.pages:
        extracted_data["document_info"]["dimensions"] = {
            "width": document.pages[0].dimension.width,
            "height": document.pages[0].dimension.height,
            "unit": document.pages[0].dimension.unit
        }

    # Den Dateinamen f√ºr die Ausgabedatei festlegen (z.B. original.pdf -> original.json)
    output_filename = f"{os.path.splitext(file_name)[0]}.json"
    
    try:
        output_bucket = storage_client.bucket(output_bucket_name)
        output_blob = output_bucket.blob(output_filename)
        
        # Die strukturierten Daten als JSON hochladen
        json_content = json.dumps(extracted_data, ensure_ascii=False, indent=2)
        output_blob.upload_from_string(json_content, content_type="application/json; charset=utf-8")
        
        print(f"üéâ Strukturierte Daten erfolgreich in gs://{output_bucket_name}/{output_filename} gespeichert.")
        print(f"üìä Extrahierte Entit√§tstypen: {list(extracted_data['entities'].keys())}")

        # --- 6. Eingangsdatei l√∂schen nach erfolgreicher Verarbeitung ---
        try:
            input_bucket = storage_client.bucket(input_bucket_name)
            input_blob = input_bucket.blob(file_name)
            input_blob.delete()
            print(f"üóëÔ∏è Eingangsdatei gs://{input_bucket_name}/{file_name} erfolgreich gel√∂scht.")
        except Exception as delete_error:
            print(f"‚ö†Ô∏è Warnung: Eingangsdatei konnte nicht gel√∂scht werden: {delete_error}")
            # Fehler beim L√∂schen ist nicht kritisch, da das Dokument bereits verarbeitet wurde

    except Exception as e:
        print(f"‚ùå Fehler beim Speichern der Ergebnisdatei: {e}")
        # Eingangsdatei NICHT l√∂schen, wenn die Verarbeitung fehlgeschlagen ist